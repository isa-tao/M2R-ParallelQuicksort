# -*- coding: utf-8 -*-
#+STARTUP: overview indent inlineimages
#+TITLE:       Laboratory Notebook for a Multi-Threaded Version of Quicksort
#+AUTHOR:      Arnaud Legrand
#+LANGUAGE:    en
#+TAGS: IMPORTANT(i) TEST(t) DEPRECATED(d) noexport(n)

* Project Overview
This project aims at providing an efficient multi-threaded
implementation of the QuickSort algorithm on multi-core machines. This
document contains my attempts to evaluate the performance of an
implementation of such code.
* General Organization
** src/
This directory comprises the parallel implementation and a standard
Makefile to compile it.
** data/
This is where raw experimental data should go. Each directory entry
comprises a set of experiments and the directory name is based on the
machine name and on the date. For example:
#+begin_src sh :results output :exports both 
echo mkdir data/`hostname`_`date +%F`
#+end_src

#+RESULTS:
: mkdir data/sama_2014-10-13

* Typical usage
** Compilation
A simple makefile with various compilation options is provided in the
src/ directory. Compilation is thus done by running the following command:
#+begin_src sh :results output :exports both 
make -C src/
#+end_src

#+RESULTS:
: make: Entering directory '/home/alegrand/Work/Documents/Enseignements/M2R_Eval_Perf_13/M2R-ParallelQuicksort/src'
: cc   -g -Wall -Wshadow -Wcast-align -Waggregate-return -Wmissing-prototypes -Wmissing-declarations -Wstrict-prototypes -Wmissing-prototypes -Wmissing-declarations -Wmissing-noreturn -Wpointer-arith -Wwrite-strings -finline-functions -O0 -pthread -lrt -std=c99  -c -o parallelQuicksort.o parallelQuicksort.c
: cc   -g -Wall -Wshadow -Wcast-align -Waggregate-return -Wmissing-prototypes -Wmissing-declarations -Wstrict-prototypes -Wmissing-prototypes -Wmissing-declarations -Wmissing-noreturn -Wpointer-arith -Wwrite-strings -finline-functions -O0 -pthread -lrt -std=c99  parallelQuicksort.o  -o parallelQuicksort 
: make: Leaving directory '/home/alegrand/Work/Documents/Enseignements/M2R_Eval_Perf_13/M2R-ParallelQuicksort/src'

** Running the code
The code is quite simple at the moment and can be run in the following way:
#+begin_src
./src/parallelQuicksort [1000000]
#+end_src
When run, the code executes initializes an array of the size given in
argument (1000000 by default) with random integer values and sorts it
using:
1. a custom sequential implementation;
2. a custom parallel implementation;
3. the libc qsort function.
Times are reported in seconds.
* Experimental Reports
** 2014-10-13
*** Initial code design
- I obtained an initial implementation from
  http://sc12.supercomputing.org/hpceducator/PythonForParallelism/codes/parallelQuicksort.c.
  According to the header, the original author is Joshua Stough from
  Washington and Lee University. I hope he will not mind that I reuse
  this piece of code for educational purposes.
- Here is a typical first execution on my laptop (an Intel(R) Core(TM)
  i7 running a Debian with Linux 3.14.15):
  #+begin_src sh :results output :exports both 
    ./src/quicksort
  #+end_src

  #+RESULTS:
  : Sequential quicksort took: 0.231571 sec.
  : at loc 506315, 5.068226e-01 < 5.068269e-01 
  : Oops, lyst did not get sorted by parallelQuicksort.
  : Parallel quicksort took: 0.161259 sec.
  : Built-in qsort took: 0.241568 sec.

  Sweet, in my first attempt, it looks like this parallel version is
  indeed running faster than then sequential one. I have to say this
  warning message is stressing me a bit though.
- On smaller instances, the code would segfault. So I reindented the
  code and thanks to valgrind and gdb, I could find what was wrong. I
  also renamed the file so that compiling is more convenient. This
  fixed the previous warning message so now everything seems fine:
  #+begin_src sh :results output :exports both 
    ./src/parallelQuicksort
  #+end_src

  #+RESULTS:
  : Sequential quicksort took: 0.239347 sec.
  : Parallel quicksort took: 0.176365 sec.
  : Built-in quicksort took: 0.244716 sec.

*** First series of experiments
Let's try to see how the three algorithms behave when changing the 
array size. Since one measurement is not enough, I run the code 5
times in a row.
#+begin_src sh foo :results output :exports both :tangle scripts/run_benchmarking.sh
  OUTPUT_DIRECTORY=data/`hostname`_`date +%F`
  mkdir -p $OUTPUT_DIRECTORY
  OUTPUT_FILE=$OUTPUT_DIRECTORY/measurements_`date +%R`.txt

  touch $OUTPUT_FILE
  for i in 100 1000 10000 100000 1000000; do
      for rep in `seq 1 5`; do
          echo "Size: $i" >> $OUTPUT_FILE;
          ./src/parallelQuicksort $i >> $OUTPUT_FILE;
      done ;
  done
#+end_src
I obtained the following [[file:data/sama_2014-10-13/measurements_03:47.txt][output]].

*** A simple plot with R
Here is a simple script to parse the results:
#+begin_src perl :results output raw :exports both :tangle scripts/csv_quicksort_extractor.pl
  use strict;

  my($line);
  my($size);

  print "Size, Type, Time\n" ;
  while($line=<>) {
      chomp $line;
      if($line =~/^Size: ([\d\.]*)$/) {
          $size = $1;
          next;
      } 
      if($line =~/^(.*) quicksort.*: ([\d\.]*) sec.$/) {
          print "$size, \"$1\", $2\n" ;
          next;
      } 
  }
#+end_src

I can then simply parse my data with the following command:

#+begin_src sh :results output :exports both 
perl scripts/csv_quicksort_extractor.pl < data/sama_2014-10-13/measurements_03\:47.txt > data/sama_2014-10-13/measurements_03\:47.csv
#+end_src

#+RESULTS:

#+begin_src R :results output graphics :file data/sama_2014-10-13/measurements_03:47.png :exports both :width 600 :height 400 :session
  df <- read.csv("data/sama_2014-10-13/measurements_03:47.csv",header=T)
  plot(df$Size,df$Time,col=c("red","blue","green")[df$Type])
#+end_src

#+RESULTS:
[[file:data/sama_2014-10-13/measurements_03:47.png]]

Well, this is not particularly nice and some may not know/like R.
*** A simple plot with gnuplot
So let's try to parse in an other way and use gnuplot:

#+begin_src perl :results output raw :exports both :tangle scripts/csv_quicksort_extractor2.pl
  use strict;

  my($line);
  my($size);
  my($seq,$par,$libc);
  print "Size, Seq, Par, Libc\n" ;
  while($line=<>) {
      chomp $line;
      if($line =~/^Size: ([\d\.]*)$/) {
          $size = $1;
          next;
      } 
      if($line =~/^Sequential quicksort.*: ([\d\.]*) sec.$/) {
          $seq=$1; next;
      } 
      if($line =~/^Parallel quicksort.*: ([\d\.]*) sec.$/) {
          $par=$1; next;
      } 
      if($line =~/^Built-in quicksort.*: ([\d\.]*) sec.$/) {
          $libc=$1; 
          print "$size, $seq, $pqr, $libc\n";
          next;
      }
  }
#+end_src

#+begin_src sh :results output raw :exports both 
  FILENAME="data/sama_2014-10-13/measurements_03:47"
  perl scripts/csv_quicksort_extractor2.pl < "$FILENAME.txt" > "${FILENAME}_wide.csv"
  echo "
    set terminal png size 600,400 
    set output '${FILENAME}_wide.png'
    set datafile separator ','
    set key autotitle columnhead
    plot '${FILENAME}_wide.csv' using 1:2 with linespoints, '' using 1:3 with linespoints, '' using 1:4 with linespoints
  " | gnuplot
  echo [[file:${FILENAME}_wide.png]]
#+end_src

#+RESULTS:
[[file:data/sama_2014-10-13/measurements_03:47_wide.png]]

Well, I'm not sure it is nicer but I have lines. A first crude
analysis seems to reveal the the parallel version is worth it for
arrays larger than 400000.


** 2014-10-29
*** First code execution on my laptop
- Here is a typical first execution on my laptop(an Intel(R) Core(TM)2
  Duo CPU P8600 2.40GHz * 2 running a Debian with Linux 3.2.0):
  =./src/parallelQuicksort= 
  It returns an error message:
  =ERROR; return code from pthread_create() is 11=
- When I run the code with a much smaller argument on my laptop:
  #+begin_src sh :results output :exports both 
    ./src/parallelQuicksort 500
  #+end_src

  #+RESULTS:
  : Sequential quicksort took: 0.000086 sec.
  : Parallel quicksort took: 0.019785 sec.
  : Built-in quicksort took: 0.000207 sec.
  We could see that parallel version is actually running slower than
  the sequential one.
- I found that the error is caused by "The system lacked the
  necessary resources to create another thread, or the system-imposed
  limit on the total number of threads in a process PTHREAD_THREADS_MAX
  would be exceeded."
  ref: http://stackoverflow.com/questions/7038586/return-code-from-pthread-create-is-11
- To increase the value of PTHREAD_THREADS_MAX per process in Linux, I
  have changed the value in the file =/proc/sys/kernel/threads-max= from
  =64513= to =120000= and have decreased stack size by the command =ulimit -v newvalue=
  ref: http://stackoverflow.com/questions/344203/maximum-number-of-threads-per-process-in-linux
  #+begin_src sh :results output :exports both
    ulimit -s 256
    ./src/parallelQuicksort
  #+end_src

  #+RESULTS:
  : Sequential quicksort took: 0.333926 sec.
  : Parallel quicksort took: 0.278951 sec.
  : Built-in quicksort took: 0.387445 sec.
  We could see that with the defaut argument 1000000, the parallel
  version is running faster than the sequential one.

*** First series of experiments
To run the series of experiments, I reused the script of Mr. Arnaud
Legrand and modified it to avoid the error =11=.
#+begin_src sh foo :results output :exports both :tangle scripts/run_benchmarking.sh
  ulimit -s 256
  OUTPUT_DIRECTORY=data/`hostname`_`date +%F`
  mkdir -p $OUTPUT_DIRECTORY
  OUTPUT_FILE=$OUTPUT_DIRECTORY/measurements_`date +%R`.txt

  touch $OUTPUT_FILE
  for i in 100 1000 10000 100000 1000000; do
      for rep in `seq 1 5`; do
          echo "Size: $i" >> $OUTPUT_FILE;
          ./src/parallelQuicksort $i >> $OUTPUT_FILE;
      done ;
  done
#+end_src
I obtained the following [[file:data/debian_2014-10-29/measurements_16:17.txt][output]].

*** Analyse the result with gnuplot
I have reused the script [[file:scripts/csv_quicksort_extractor2.pl][csv_quicksort_extractor2.pl]]
#+begin_src sh :results output raw :exports both 
  FILENAME="data/debian_2014-10-29/measurements_16:17"
  perl scripts/csv_quicksort_extractor2.pl < "$FILENAME.txt" > "${FILENAME}_wide.csv"
  echo "
    set terminal png size 600,400 
    set output '${FILENAME}_wide.png'
    set datafile separator ','
    set key autotitle columnhead
    plot '${FILENAME}_wide.csv' using 1:2 with linespoints, '' using 1:3 with linespoints, '' using 1:4 with linespoints
  " | gnuplot
  echo [[file:${FILENAME}_wide.png]]
#+end_src

#+RESULTS:
[[file:data/debian_2014-10-29/measurements_16:17_wide.png]]

From this first analyse, it seems that the parallel version is running
faster for arrays larger than 600000.
*** code execution on Grid5000
Run the series of experiments on Grid5000:
#+begin_src sh foo :results output :exports both :tangle scripts/run_benchmarking.sh
  OUTPUT_DIRECTORY=data/`hostname`_`date +%F`
  mkdir -p $OUTPUT_DIRECTORY
  OUTPUT_FILE=$OUTPUT_DIRECTORY/measurements_`date +%R`.txt

  touch $OUTPUT_FILE
  for i in 100 1000 10000 100000 1000000; do
      for rep in `seq 1 5`; do
          echo "Size: $i" >> $OUTPUT_FILE;
          ./src/parallelQuicksort $i >> $OUTPUT_FILE;
      done ;
  done
#+end_src
I obtained the following [[file:data/sagittaire-78.lyon.grid5000.fr_2014-10-29/measurements_17:47.txt][output]], and used the same script to parse
this result:
#+begin_src sh :results output raw :exports both 
  FILENAME="data/sagittaire-78.lyon.grid5000.fr_2014-10-29/measurements_17:47"
  perl scripts/csv_quicksort_extractor2.pl < "$FILENAME.txt" > "${FILENAME}_wide.csv"
  echo "
    set terminal png size 600,400 
    set output '${FILENAME}_wide.png'
    set datafile separator ','
    set key autotitle columnhead
    plot '${FILENAME}_wide.csv' using 1:2 with linespoints, '' using 1:3 with linespoints, '' using 1:4 with linespoints
  " | gnuplot
  echo [[file:${FILENAME}_wide.png]]
#+end_src

#+RESULTS:
[[file:data/sagittaire-78.lyon.grid5000.fr_2014-10-29/measurements_17:47_wide.png]]

We could find that the performance of the sequential paralle version
and sequential version didn't change much, but the performance of libc
function is better than others unlike the experiments on my local
machine.





** 2014-11-02
*** Experiments for arrays between 500000 and 800000
From the series of experiments before, we could find that the parallel
version starts to perform better for arrays larger than 600000, so I
want to do more experiments for arrays between 500000 and 800000 to
verify the conclusion.
The experiments is done on my local machine.
#+begin_src sh foo :results output :exports both :tangle scripts/run_benchmarking.sh
  ulimit -s 256
  OUTPUT_DIRECTORY=data/`hostname`_`date +%F`
  mkdir -p $OUTPUT_DIRECTORY
  OUTPUT_FILE=$OUTPUT_DIRECTORY/measurements_`date +%R`.txt

  touch $OUTPUT_FILE
  for i in 500000 550000 600000 650000 700000 750000 800000; do
      for rep in `seq 1 5`; do
          echo "Size: $i" >> $OUTPUT_FILE;
          ./src/parallelQuicksort $i >> $OUTPUT_FILE;
      done ;
  done
#+end_src

#+RESULTS:

Here is the [[file:data/debian_2014-11-02/measurements_20:32.txt][output]]. Let's parse this result:
#+begin_src sh :results output raw :exports both 
  FILENAME="data/debian_2014-11-02/measurements_20:32"
  perl scripts/csv_quicksort_extractor2.pl < "$FILENAME.txt" > "${FILENAME}_wide.csv"
  echo "
    set terminal png size 600,400 
    set output '${FILENAME}_wide.png'
    set datafile separator ','
    set key autotitle columnhead
    plot '${FILENAME}_wide.csv' using 1:2 with linespoints, '' using 1:3 with linespoints, '' using 1:4 with linespoints
  " | gnuplot
  echo [[file:${FILENAME}_wide.png]]
#+end_src

#+RESULTS:
[[file:data/debian_2014-11-02/measurements_20:32_wide.png]]




** 2014-11-09
*** information collection
The parameters that may influence the experiment results:
- Host information
  - CPU type
  - cache size
  - number of threads
  - RAM
- OS kernel version
- workload
  - dedicated or not
  - deamons
- Compiler
  - version
  - options
- Input
Here I think that the most important parameters that we should study
are: 
=- the number of processors p; 
 - number of threads; 
 - the input size(the size of the array n).
 - the method to calculate the time(in the current version the
   function gettimeofday() is used)=
First I want to fix the experiment environment as my local machine(p=2),
and fix the number of threads, to study the relation between
execution time T and the size of the array n.
*** relation between T and n
I want to find whether it's the linear relation between T and n, in the
initial experiment, the array size use log-scale, so I want to change
it to be linear. From the experiment before, I redo the experiment for the size of
arrays between 100000 and 800000.

#+begin_src sh foo :results output :exports both :tangle scripts/run_benchmarking.sh
  ulimit -s 256
  OUTPUT_DIRECTORY=data/`hostname`_`date +%F`
  mkdir -p $OUTPUT_DIRECTORY
  OUTPUT_FILE=$OUTPUT_DIRECTORY/measurements_`date +%R`.txt

  touch $OUTPUT_FILE
  for i in `seq 100000 100000 800000`; do
      for rep in `seq 1 10`; do
          echo "Size: $i" >> $OUTPUT_FILE;
          ./src/parallelQuicksort $i >> $OUTPUT_FILE;
      done ;
  done
#+end_src

#+RESULTS:

Here is the [[file:data/debian_2014-11-09/measurements_19:07.txt][output]]. The graphic before is not quite clear, so I choose
to visualize it using ggplot2.

#+begin_src sh :results output raw :exports both 
  FILENAME="data/debian_2014-11-09/measurements_19:07"
  perl scripts/csv_quicksort_extractor.pl < "$FILENAME.txt" > "${FILENAME}.csv"
#+end_src

#+RESULTS:
 
#+begin_src R :results output graphics :file data/debian_2014-11-09/measurements_19:07.png :exports both :width 600 :height 400
library(plyr)
library(ggplot2)
df <- read.csv("~/Dropbox/mosig/performance evaluation/M2R-ParallelQuicksort/data/debian_2014-11-09/measurements_19:07.csv")
splitDf <- ddply(df, .(Size, Type), summarize, mean=mean(Time), sd=sd(Time))
p <- ggplot(splitDf, aes(x=Size, y=mean, colour=Type))
limits <- aes(ymax = mean + sd, ymin=mean - sd)
p + geom_line() + geom_errorbar(limits, width=0.2) + labs(x = "Array Size", y = "Execution Time")
#+end_src

#+RESULTS:
[[file:data/debian_2014-11-09/measurements_19:07.png]]

ref:
http://stats.stackexchange.com/questions/8225/how-to-summarize-data-by-group-in-r #How
to summarize data by group in R(using ddply or data.table)
http://docs.ggplot2.org/current/geom_errorbar.html #error bars
http://docs.ggplot2.org/current/theme.html #Change the axis labels

From the [[file:data/debian_2014-11-09/measurements_19:07.png][graphic]], we could see that the relation between the data set
behaves like a linear curve, and the parallel version turns to perform
best when array size is bigger than 600000. But we should notice that
the variation seems large when array size is bigger than 700000.
  
*** TODO
- find how to change number of processors, and number of threads to
  study their relations with the performance.
- find other methods to calculate the time.

** 2014-11-15
In the course of 2014-11-10, the professor talked about the confidence
interval of the mesured random values. In the diagram that I made last
week, I have used [μ - σ, μ + σ] as the confidence interval which is
not accurate, so I want to improve it.
*** Histogram
First, I decide to increase the number of replicates of the experiment
to 30, and to study its histogram.
#+begin_src sh foo :results output :exports both :tangle scripts/run_benchmarking.sh
  ulimit -s 256
  OUTPUT_DIRECTORY=data/`hostname`_`date +%F`
  mkdir -p $OUTPUT_DIRECTORY
  OUTPUT_FILE=$OUTPUT_DIRECTORY/measurements_`date +%R`.txt

  touch $OUTPUT_FILE
  for i in `seq 100000 100000 800000`; do
      for rep in `seq 1 30`; do
          echo "Size: $i" >> $OUTPUT_FILE;
          ./src/parallelQuicksort $i >> $OUTPUT_FILE;
      done ;
  done
#+end_src

#+RESULTS:

Here is the [[file:data/debian_2014-11-15/measurements_23:25.txt][output]].

#+begin_src sh :results output raw :exports both 
  FILENAME="data/debian_2014-11-15/measurements_23:25"
  perl scripts/csv_quicksort_extractor.pl < "$FILENAME.txt" > "${FILENAME}.csv"
#+end_src

#+RESULTS:
 
#+begin_src R :results output graphics :file data/debian_2014-11-15/measurements_23:25.png :exports both :width 800 :height 400
library(plyr)
library(ggplot2)
df <- read.csv("~/Dropbox/mosig/performance evaluation/M2R-ParallelQuicksort/data/debian_2014-11-15/measurements_23:25.csv")
s <- subset(df, Size == 800000)
m<-ggplot(s,aes(x=Time))
m+geom_histogram() + facet_grid(~Type)
#+end_src

#+RESULTS:
[[file:data/debian_2014-11-15/measurements_23:25.png]]

We could see that the version sequential is most close to the normal
distribution, the version Parallel is least close to the normal
distribution. So I try to increase the number of replicates of the experiment
to 50, and to study its histogram.
#+begin_src sh foo :results output :exports both :tangle scripts/run_benchmarking.sh
  ulimit -s 256
  OUTPUT_DIRECTORY=data/`hostname`_`date +%F`
  mkdir -p $OUTPUT_DIRECTORY
  OUTPUT_FILE=$OUTPUT_DIRECTORY/measurements_`date +%R`.txt

  touch $OUTPUT_FILE
  for i in `seq 100000 100000 800000`; do
      for rep in `seq 1 50`; do
          echo "Size: $i" >> $OUTPUT_FILE;
          ./src/parallelQuicksort $i >> $OUTPUT_FILE;
      done ;
  done
#+end_src

#+RESULTS:

#+begin_src sh :results output raw :exports both 
  FILENAME="data/debian_2014-11-15/measurements_23:52"
  perl scripts/csv_quicksort_extractor.pl < "$FILENAME.txt" > "${FILENAME}.csv"
#+end_src

#+RESULTS:
 
#+begin_src R :results output graphics :file data/debian_2014-11-15/measurements_23:52.png :exports both :width 800 :height 400
library(plyr)
library(ggplot2)
df <- read.csv("~/Dropbox/mosig/performance evaluation/M2R-ParallelQuicksort/data/debian_2014-11-15/measurements_23:52.csv")
s <- subset(df, Size == 800000)
m<-ggplot(s,aes(x=Time))
m+geom_histogram() + facet_grid(~Type)
#+end_src

#+RESULTS:
[[file:data/debian_2014-11-15/measurements_23:52.png]]

The result seems to be acceptable now.

*** Confidence Intervals
Now I change the confidence interval to be  [μ - 2σ/√50, μ + 2σ/√50],
and redraw the diagram to compare the three versions.
#+begin_src R :results output graphics :file data/debian_2014-11-15/measurements_23:52_compare.png :exports both :width 600 :height 400
library(plyr)
library(ggplot2)
df <- read.csv("~/Dropbox/mosig/performance evaluation/M2R-ParallelQuicksort/data/debian_2014-11-15/measurements_23:52.csv")
splitDf <- ddply(df, .(Size, Type), summarize, mean=mean(Time), sd=sd(Time))
p <- ggplot(splitDf, aes(x=Size, y=mean, colour=Type))
limits <- aes(ymax = mean + 2 * sd/(50^0.5), ymin=mean - 2 * sd/(50^0.5))
p + geom_line() + geom_errorbar(limits, width=0.2) + labs(x = "Array Size", y = "Execution Time")
#+end_src

#+RESULTS:
[[file:data/debian_2014-11-15/measurements_23:52_compare.png]]

Now we could see that for the array size of 600000, the confidence
intervals of version of parallel and sequential don't overlap, so we
could say that at this time the parallel version is better with the
probability over 90%.

*** TODO
- find how to change number of processors, and number of threads to
  study their relations with the performance.
- find other methods to calculate the time.
- compute the speedup and efficiency for the parallel versions.
